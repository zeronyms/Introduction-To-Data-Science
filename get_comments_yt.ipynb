{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great stuff, thanks! I wonder if I can retrieve all comments related to specific topic or video title (from multiple videos) or video link is required parameter? Thanks again.\n",
      "thankyou adam\n",
      "good video\n",
      "How many maximum comments can we pull at a time ? I can pull 20 commments . But I want to get more .. Help us\n",
      "Only the top level comments are returned through the API. How do we get all the comments?\n",
      "How do I put the comments into spreadsheet?\n",
      "Finally got around to making the advanced video. Find out how to Scrape all comments and attributes here <a href=\"https://youtu.be/0FtcHjI5lmw?si=QtUjhVqTXJS3YKZP\">https://youtu.be/0FtcHjI5lmw?si=QtUjhVqTXJS3YKZP</a>\n",
      "wow\n",
      "works with live video ?\n",
      "Great video! will it also scrape Instagram videos?\n",
      "Thank you so much! Great video. Looking forward to the second part\n",
      "Great video! Can you make an explain video on how to write the code for these pull comments? Why do we need these libraries? Where can we find the format &#39; youtube = googleapiclient.discovery.build&#39;? Thank you vary much!\n",
      "I tried to make a loop to get all the videos from a channel and scrape all the comments. However, it gave the following error &quot;the request cannot be completed because you have exceeded your quota&quot; - From what I understand, the API has a daily request limit, is that it? - In fact, when the video has a private comment, the request returns 403.\n",
      "Hi, i Found this video to be very helpfull in my research, I am wondering if there is a way to get the result to be top comment instead of latest comment ?\n",
      "Does this work with Youtube shorts?\n",
      "No te olvides de continuar este proyecto y agregar mas paginacion a los 100 comentarios por favor\n",
      "question, i&#39;m currently using the youtube api to get comments from a video, however, for some reason, the api also gets the comments deleted by the channel&#39;s owner (like scam comments or hateful comments that were deleted from the video comments for obvious reasons) is there any way to remove those type of comments from the api request without having to find every single hateful comment&#39;s id and remove them manually?\n",
      "But it doesn&#39;t show all the comments, even changing the &quot;maxResults&quot; variable :(\n",
      "Thank you so much Adam .It is really very helpful.\n",
      "Very helpful video. I was doing web scrapping using bs4 and selenium, but this API way is very good and easy.\n",
      "Can i scrap any youtube video? Or just the video that i have on my channel?\n",
      "hi, how to fetch real time data from you tube, please guide me\n",
      "Thanks, easy peasy! Is there a way we can have a column for language?\n",
      "Thanks, Adam. This was easy and straightforward. <br>Is it possible to get up to 1000 comments using this cos it seems there&#39;s a cap of 100 even after I changed the limit\n",
      "Just a question, do we have to pay to use google cloud platform?\n",
      "Thank you, Adam! Very straightforward!\n",
      "hey Adam,<br>Thank you for this video, I will really appreciate if you upload another videos[Advance] on the same topic. Waiting........\n",
      "Thank you for this Adam. Great tuorial\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "api_service_name = 'youtube'\n",
    "api_version = 'v3'\n",
    "developer_key = 'AIzaSyACM3jJ5dy3rTBWYzR8hBvI_OTP4FoBoXo'\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey= developer_key\n",
    ")\n",
    "request = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId='SIm2W9TtzR0',\n",
    "    maxResults=100\n",
    ")\n",
    "response = request.execute()\n",
    "\n",
    "for item in response['items']:\n",
    "    print(item['snippet']['topLevelComment']['snippet']['textDisplay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             author          published_at  \\\n",
      "0                  @user-hx2xi1bs1c  2024-01-03T15:15:32Z   \n",
      "1                    @hanifbudiarti  2023-12-04T14:40:36Z   \n",
      "2                 @mayurhanwate6012  2023-11-13T19:13:51Z   \n",
      "3                @ketonesgaming1121  2023-11-10T19:40:02Z   \n",
      "4                   @soumilyade1057  2023-11-10T04:36:31Z   \n",
      "5            @crisearlbalangyao3190  2023-11-02T04:42:15Z   \n",
      "6                @analyticswithadam  2023-10-31T09:22:11Z   \n",
      "7         @shuvodeeptalukdarsh-2143  2023-10-30T15:13:26Z   \n",
      "8                   @mudeamemdm8299  2023-10-26T22:02:12Z   \n",
      "9                   @shahirnaga4507  2023-10-22T07:57:21Z   \n",
      "10           @JavierRodriguez-wq3yw  2023-10-14T11:02:05Z   \n",
      "11                @GraceZhang-oo5sf  2023-10-11T19:22:05Z   \n",
      "12                       @JeanSuman  2023-10-03T15:01:35Z   \n",
      "13        @hernandosangkanparan5543  2023-09-30T15:38:26Z   \n",
      "14              @SuneetDSilva-ll9ij  2023-09-28T18:44:04Z   \n",
      "15                      @haroldh200  2023-09-28T05:03:19Z   \n",
      "16                      @zapalblizh  2023-09-14T14:43:45Z   \n",
      "17                @estebanlopez5372  2023-09-09T11:55:34Z   \n",
      "18                 @user-fq3eb4hq8r  2023-09-07T16:45:12Z   \n",
      "19                      @romanr8668  2023-08-17T13:42:43Z   \n",
      "20                   @gabiru-danger  2023-08-07T03:45:07Z   \n",
      "21              @preetandaisiri9967  2023-08-06T17:10:34Z   \n",
      "22  @letconsultingpartnermexico6390  2023-08-02T23:50:21Z   \n",
      "23                      @paulnnakwe  2023-07-18T16:58:47Z   \n",
      "24               @hatingtoxicpeople  2023-07-16T15:35:01Z   \n",
      "25               @hatingtoxicpeople  2023-07-16T15:19:28Z   \n",
      "26                  @zoraizelya3975  2023-07-13T18:58:43Z   \n",
      "27                       @oraclesql  2023-07-12T19:32:27Z   \n",
      "\n",
      "              updated_at  like_count  \\\n",
      "0   2024-01-03T15:15:32Z           0   \n",
      "1   2023-12-04T14:40:36Z           1   \n",
      "2   2023-11-13T19:13:51Z           1   \n",
      "3   2023-11-10T19:40:02Z           0   \n",
      "4   2023-11-10T04:36:31Z           0   \n",
      "5   2023-11-02T04:42:15Z           0   \n",
      "6   2023-10-31T09:23:11Z           1   \n",
      "7   2023-10-30T15:13:26Z           0   \n",
      "8   2023-10-26T22:02:12Z           0   \n",
      "9   2023-10-22T07:57:21Z           0   \n",
      "10  2023-10-14T11:02:05Z           0   \n",
      "11  2023-10-11T19:22:05Z           0   \n",
      "12  2023-10-03T15:02:40Z           0   \n",
      "13  2023-09-30T15:38:26Z           0   \n",
      "14  2023-09-28T18:44:04Z           0   \n",
      "15  2023-09-28T05:03:19Z           0   \n",
      "16  2023-09-14T14:43:45Z           1   \n",
      "17  2023-09-09T11:55:34Z           1   \n",
      "18  2023-09-07T16:45:12Z           2   \n",
      "19  2023-08-17T13:42:43Z           0   \n",
      "20  2023-08-07T03:45:07Z           1   \n",
      "21  2023-08-06T17:10:34Z           0   \n",
      "22  2023-08-02T23:50:21Z           0   \n",
      "23  2023-07-18T16:58:47Z           0   \n",
      "24  2023-07-16T15:35:01Z           0   \n",
      "25  2023-07-16T15:19:28Z           1   \n",
      "26  2023-07-13T18:58:43Z           0   \n",
      "27  2023-07-12T19:32:27Z           1   \n",
      "\n",
      "                                                 text  \n",
      "0   Great stuff, thanks! I wonder if I can retriev...  \n",
      "1                                       thankyou adam  \n",
      "2                                          good video  \n",
      "3   How many maximum comments can we pull at a tim...  \n",
      "4   Only the top level comments are returned throu...  \n",
      "5         How do I put the comments into spreadsheet?  \n",
      "6   Finally got around to making the advanced vide...  \n",
      "7                                                 wow  \n",
      "8                             works with live video ?  \n",
      "9   Great video! will it also scrape Instagram vid...  \n",
      "10  Thank you so much! Great video. Looking forwar...  \n",
      "11  Great video! Can you make an explain video on ...  \n",
      "12  I tried to make a loop to get all the videos f...  \n",
      "13  Hi, i Found this video to be very helpfull in ...  \n",
      "14                Does this work with Youtube shorts?  \n",
      "15  No te olvides de continuar este proyecto y agr...  \n",
      "16  question, i&#39;m currently using the youtube ...  \n",
      "17  But it doesn&#39;t show all the comments, even...  \n",
      "18  Thank you so much Adam .It is really very help...  \n",
      "19  Very helpful video. I was doing web scrapping ...  \n",
      "20  Can i scrap any youtube video? Or just the vid...  \n",
      "21  hi, how to fetch real time data from you tube,...  \n",
      "22  Thanks, easy peasy! Is there a way we can have...  \n",
      "23  Thanks, Adam. This was easy and straightforwar...  \n",
      "24  Just a question, do we have to pay to use goog...  \n",
      "25             Thank you, Adam! Very straightforward!  \n",
      "26  hey Adam,<br>Thank you for this video, I will ...  \n",
      "27             Thank you for this Adam. Great tuorial  \n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "\n",
    "api_service_name='youtube'\n",
    "api_version='v3'\n",
    "developer_key='AIzaSyACM3jJ5dy3rTBWYzR8hBvI_OTP4FoBoXo'\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name,api_version,developerKey=developer_key\n",
    ")\n",
    "request = youtube.commentThreads().list(\n",
    "    part='snippet',\n",
    "    videoId='SIm2W9TtzR0',\n",
    "    maxResults=100\n",
    ")\n",
    "response = request.execute()\n",
    "comments = []\n",
    "\n",
    "for item in response['items']:\n",
    "    comment = item['snippet']['topLevelComment']['snippet']\n",
    "    comments.append([\n",
    "        comment['authorDisplayName'],\n",
    "        comment['publishedAt'],\n",
    "        comment['updatedAt'],\n",
    "        comment['likeCount'],\n",
    "        comment['textDisplay'],\n",
    "    ])\n",
    "\n",
    "df=pd.DataFrame(comments, columns=['author', 'published_at', 'updated_at', 'like_count', 'text'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
