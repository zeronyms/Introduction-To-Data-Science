{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scrapping data dengan Youtube API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Komentar\n",
      "1          Dosen ketemu pertanyaan y jawabanny bagus,.\n",
      "2    Coba perhatikan Anies, bukan senyum melaikan A...\n",
      "3    Waktu kemarin debat pertama menurut saya pak A...\n",
      "4    Nilai debat kemarin \\nPaslon 1 = 120\\nPaslom 2...\n",
      "5    Suka banget lihat percakapan dan bahasa tubuh ...\n",
      "..                                                 ...\n",
      "371  Jawaban PRABOWO jelas sesuai pngalaman Beliau ...\n",
      "372  bayi jg jdi pemimpin...asal ayah presiden kano...\n",
      "373                                            ttep 02\n",
      "374  jagoan lu no 2 levelnya rendah jadi kasih pert...\n",
      "375  prabowo kebanyakan kata-kata saudara-saudara s...\n",
      "\n",
      "[375 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# web scraping\n",
    "import pandas as pd\n",
    "import os\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "os.environ['OAUTHLIB_INSECURE_TRANSPORT'] = '1'\n",
    "youTubeApiKey = 'AIzaSyACM3jJ5dy3rTBWYzR8hBvI_OTP4FoBoXo'\n",
    "youtube = build('youtube', 'v3', developerKey=youTubeApiKey)\n",
    "data_video = [[\"Nama\", \"Komentar\", \"Waktu\", \"Likes\", \"Reply Count\"]]\n",
    "\n",
    "def get_all_comment(video_use):\n",
    "    param_comment = youtube.commentThreads().list(part=\"snippet\", videoId=video_use, maxResults=\"100\", textFormat=\"plainText\")\n",
    "    \n",
    "    while True:\n",
    "        data_comment = param_comment.execute()\n",
    "\n",
    "        for i in data_comment[\"items\"]:\n",
    "            name = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"authorDisplayName\"]\n",
    "            comment = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "            published_at = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"publishedAt\"]\n",
    "            likes = i[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"likeCount\"]\n",
    "            replies = i[\"snippet\"][\"totalReplyCount\"]\n",
    "            data_video.append([name, comment, published_at, likes, replies])\n",
    "\n",
    "            totalReplyCount = i[\"snippet\"][\"totalReplyCount\"]\n",
    "            if totalReplyCount > 0:\n",
    "                parent = i[\"snippet\"][\"topLevelComment\"][\"id\"]\n",
    "                param_replies = youtube.comments().list(part=\"snippet\", maxResults=\"100\", parentId=parent, textFormat=\"plainText\")\n",
    "                data_replies = param_replies.execute()\n",
    "                for reply in data_replies[\"items\"]:\n",
    "                    reply_name = reply[\"snippet\"][\"authorDisplayName\"]\n",
    "                    reply_comment = reply[\"snippet\"][\"textDisplay\"]\n",
    "                    reply_published_at = reply[\"snippet\"][\"publishedAt\"]\n",
    "                    reply_likes = reply[\"snippet\"][\"likeCount\"]\n",
    "                    reply_replies = \"\"\n",
    "                    data_video.append([reply_name, reply_comment, reply_published_at, reply_likes, reply_replies])\n",
    "\n",
    "        if 'nextPageToken' in data_comment:\n",
    "            nextToken = data_comment['nextPageToken']\n",
    "            param_comment = youtube.commentThreads().list(part=\"snippet\", videoId=video_use, maxResults=\"100\", textFormat=\"plainText\", pageToken=nextToken)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "get_all_comment(\"qtBuBYBsnTw\")\n",
    "df = pd.DataFrame({\"Nama\": [i[0] for i in data_video], \n",
    "                   \"Komentar\": [i[1] for i in data_video], \n",
    "                   \"Waktu\": [i[2] for i in data_video],\n",
    "                   \"Likes\": [i[3] for i in data_video], \n",
    "                   \"Reply Count\": [i[4] for i in data_video]}\n",
    "                 )\n",
    "df.to_csv(\"Hasil Scrape.csv\", index=False, header=False)\n",
    "\n",
    "df_data = pd.DataFrame({\"Nama\": [i[0] for i in data_video], \n",
    "                        \"Komentar\": [i[1] for i in data_video], \n",
    "                        \"Waktu\": [i[2] for i in data_video]}\n",
    "                      )\n",
    "df_show = df_data.copy()\n",
    "df_show = df_show.drop(0)\n",
    "df = df_data.drop(['Nama', 'Waktu'], axis=1)\n",
    "df = df.drop(0)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Text Preprocessing\n",
    "<h3> 1. Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1              dosen ketemu pertanyaan y jawabanny bagus\n",
       "2      coba perhatikan anies bukan senyum melaikan an...\n",
       "3      waktu kemarin debat pertama menurut saya pak a...\n",
       "4      nilai debat kemarin \\npaslon   \\npaslom \\npasl...\n",
       "5      suka banget lihat percakapan dan bahasa tubuh ...\n",
       "                             ...                        \n",
       "371    jawaban prabowo jelas sesuai pngalaman beliau ...\n",
       "372        bayi jg jdi pemimpinasal ayah presiden kanoha\n",
       "373                                                ttep \n",
       "374    jagoan lu no  levelnya rendah jadi kasih perta...\n",
       "375    prabowo kebanyakan katakata saudarasaudara sek...\n",
       "Name: Komentar, Length: 375, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# MENGUBAH TEXT MENJADI LOWERCASE\n",
    "df['Komentar'] = df['Komentar'].str.lower()\n",
    "df['Komentar'] = df['Komentar'].str.strip()\n",
    "# MENGHAPUS ANGKA\n",
    "df['Komentar'] = df['Komentar'].apply(lambda x: re.sub(r'\\d', '', x))\n",
    "# MENGHAPUS TANDA BACA\n",
    "df['Komentar'] = df['Komentar'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "df['Komentar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "1     [dosen, ketemu, pertanyaan, y, jawabanny, bagus]\n",
      "2    [coba, perhatikan, anies, bukan, senyum, melai...\n",
      "3    [waktu, kemarin, debat, pertama, menurut, saya...\n",
      "4    [nilai, debat, kemarin, paslon, paslom, paslon...\n",
      "5    [suka, banget, lihat, percakapan, dan, bahasa,...\n",
      "Name: komentar_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "df['komentar_tokens'] = df['Komentar'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(df['komentar_tokens'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1                 [dosen, ketemu, y, jawabanny, bagus]\n",
      "2    [coba, perhatikan, anies, senyum, melaikan, an...\n",
      "3    [kemarin, debat, anies, bagus, kali, debat, ke...\n",
      "4    [nilai, debat, kemarin, paslon, paslom, paslon...\n",
      "5    [suka, banget, lihat, percakapan, bahasa, tubu...\n",
      "Name: komentar_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "# txt_stopword = pd.read_csv(\"stopwords.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# # convert stopword string to list & append additional stopword\n",
    "# list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "df['komentar_tokens_WSW'] = df['komentar_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(df['komentar_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"Text_Preprocessing.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizad_word = pd.read_excel(\"normalisasi.xlsx\")\n",
    "\n",
    "# normalizad_word_dict = {}\n",
    "\n",
    "# for index, row in normalizad_word.iterrows():\n",
    "#     if row[0] not in normalizad_word_dict:\n",
    "#         normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "# def normalized_term(document):\n",
    "#     return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
    "\n",
    "# df['tweet_normalized'] = df['komentar_tokens_WSW'].apply(normalized_term)\n",
    "\n",
    "# df['tweet_normalized'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stemming\n",
    "\n",
    "# # import Sastrawi package\n",
    "# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# import swifter\n",
    "\n",
    "\n",
    "# # create stemmer\n",
    "# factory = StemmerFactory()\n",
    "# stemmer = factory.create_stemmer()\n",
    "\n",
    "# # stemmed\n",
    "# def stemmed_wrapper(term):\n",
    "#     return stemmer.stem(term)\n",
    "\n",
    "# term_dict = {}\n",
    "\n",
    "# for document in df['komentar_tokens_WSW']:\n",
    "#     for term in document:\n",
    "#         if term not in term_dict:\n",
    "#             term_dict[term] = ' '\n",
    "            \n",
    "# print(len(term_dict))\n",
    "# print(\"------------------------\")\n",
    "\n",
    "# for term in term_dict:\n",
    "#     term_dict[term] = stemmed_wrapper(term)\n",
    "#     print(term,\":\" ,term_dict[term])\n",
    "    \n",
    "# print(term_dict)\n",
    "# print(\"------------------------\")\n",
    "\n",
    "\n",
    "# # apply stemmed term to dataframe\n",
    "# def get_stemmed_term(document):\n",
    "#     return [term_dict[term] for term in document]\n",
    "\n",
    "# df['komentar_tokens_stemmed'] = df['komentar_tokens_WSW'].swifter.apply(get_stemmed_term)\n",
    "# print(df['komentar_tokens_stemmed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
